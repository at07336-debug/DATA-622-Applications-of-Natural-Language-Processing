{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.2 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCMgtDxNcacb",
        "outputId": "01491ab6-1927-4d89-f7e4-fea3345c31ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.3 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization  \n",
        "Tokenize both sentences into words using spaCy. Print the list of tokens for each sentence.\n",
        "Also use the benepar library."
      ],
      "metadata": {
        "id": "C779Em3qeqw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "import nltk\n",
        "from nltk import Tree\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "except OSError:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "benepar.download('benepar_en3', quiet=True)\n",
        "bp = benepar.Parser(\"benepar_en3\")\n",
        "\n",
        "sentence1 = (\n",
        "    \"Four score and seven years ago our fathers brought forth on this continent, \"\n",
        "    \"a new nation, conceived in Liberty, and dedicated to the proposition \"\n",
        "    \"that all men are created equal.\"\n",
        ")\n",
        "sentence2 = (\n",
        "    \"Now we are engaged in a great civil war, testing whether that nation, \"\n",
        "    \"or any nation so conceived and so dedicated, can long endure.\"\n",
        ")\n",
        "\n",
        "doc1 = nlp(sentence1)\n",
        "doc2 = nlp(sentence2)\n",
        "tree1 = bp.parse(sentence1)\n",
        "tree2 = bp.parse(sentence2)\n",
        "\n",
        "print(\"\\n--- Sentence 1 Tokens (spaCy) ---\")\n",
        "print([token.text for token in doc1])\n",
        "\n",
        "print(\"\\n--- Sentence 2 Tokens (spaCy) ---\")\n",
        "print([token.text for token in doc2])\n",
        "\n",
        "print(\"\\n--- Sentence 1 Tokens (benepar leaves) ---\")\n",
        "print(tree1.leaves())\n",
        "\n",
        "print(\"\\n--- Sentence 2 Tokens (benepar leaves) ---\")\n",
        "print(tree2.leaves())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VV9TOoKdIEz",
        "outputId": "6f6b6c2e-08cb-4c1a-88e7-b14898ec358e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence 1 Tokens (spaCy) ---\n",
            "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.']\n",
            "\n",
            "--- Sentence 2 Tokens (spaCy) ---\n",
            "['Now', 'we', 'are', 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.']\n",
            "\n",
            "--- Sentence 1 Tokens (benepar leaves) ---\n",
            "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.']\n",
            "\n",
            "--- Sentence 2 Tokens (benepar leaves) ---\n",
            "['Now', 'we', 'are', 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Part-of-Speech Tagging  \n",
        "Print the part-of-speech (POS) tag for each token in the first sentence."
      ],
      "metadata": {
        "id": "qDhdPplne1UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'Token':<20} {'Coarse POS':<12} {'Fine-grained Tag'}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc1:\n",
        "    print(f\"{token.text:<20} {token.pos_:<12} {token.tag_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7mZzcWedzYB",
        "outputId": "c205d021-a861-46ed-af91-01a9dd2804de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token                Coarse POS   Fine-grained Tag\n",
            "--------------------------------------------------\n",
            "Four                 NUM          CD\n",
            "score                NOUN         NN\n",
            "and                  CCONJ        CC\n",
            "seven                NUM          CD\n",
            "years                NOUN         NNS\n",
            "ago                  ADV          RB\n",
            "our                  PRON         PRP$\n",
            "fathers              NOUN         NNS\n",
            "brought              VERB         VBD\n",
            "forth                ADP          RP\n",
            "on                   ADP          IN\n",
            "this                 DET          DT\n",
            "continent            NOUN         NN\n",
            ",                    PUNCT        ,\n",
            "a                    DET          DT\n",
            "new                  ADJ          JJ\n",
            "nation               NOUN         NN\n",
            ",                    PUNCT        ,\n",
            "conceived            VERB         VBN\n",
            "in                   ADP          IN\n",
            "Liberty              PROPN        NNP\n",
            ",                    PUNCT        ,\n",
            "and                  CCONJ        CC\n",
            "dedicated            VERB         VBN\n",
            "to                   ADP          IN\n",
            "the                  DET          DT\n",
            "proposition          NOUN         NN\n",
            "that                 SCONJ        IN\n",
            "all                  DET          DT\n",
            "men                  NOUN         NNS\n",
            "are                  AUX          VBP\n",
            "created              VERB         VBN\n",
            "equal                ADJ          JJ\n",
            ".                    PUNCT        .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dependency Parsing  \n",
        "Print the dependency relation and head word for each token in the second sentence."
      ],
      "metadata": {
        "id": "QPdHZS9Ue9bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'Token':<20} {'Dep Relation':<16} {'Head Word'}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc2:\n",
        "    print(f\"{token.text:<20} {token.dep_:<16} {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63aG_2ved5z1",
        "outputId": "ac2e92b0-bd3c-4270-87cb-22ec92300416"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token                Dep Relation     Head Word\n",
            "--------------------------------------------------\n",
            "Now                  advmod           engaged\n",
            "we                   nsubjpass        engaged\n",
            "are                  auxpass          engaged\n",
            "engaged              ROOT             engaged\n",
            "in                   prep             engaged\n",
            "a                    det              war\n",
            "great                amod             war\n",
            "civil                amod             war\n",
            "war                  pobj             in\n",
            ",                    punct            engaged\n",
            "testing              advcl            engaged\n",
            "whether              mark             conceived\n",
            "that                 det              nation\n",
            "nation               nsubj            conceived\n",
            ",                    punct            nation\n",
            "or                   cc               nation\n",
            "any                  det              nation\n",
            "nation               conj             nation\n",
            "so                   advmod           conceived\n",
            "conceived            ccomp            testing\n",
            "and                  cc               conceived\n",
            "so                   advmod           dedicated\n",
            "dedicated            conj             conceived\n",
            ",                    punct            testing\n",
            "can                  aux              endure\n",
            "long                 advmod           endure\n",
            "endure               conj             testing\n",
            ".                    punct            engaged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Constituent Parsing  \n",
        "Using the NLTK and benepar libraries, print the constituency (phrase structure) parse tree\n",
        "of the first sentence."
      ],
      "metadata": {
        "id": "WL-O2rfHfA73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree1.pretty_print()\n",
        "\n",
        "print(\"\\nRaw parse string:\")\n",
        "print(str(tree1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOOc9avRWu17",
        "outputId": "c0f943a8-f58f-4918-af85-51fa62e64199"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                          TOP                                                                                                                  \n",
            "                                                                                                           |                                                                                                                    \n",
            "                                                                                                           S                                                                                                                   \n",
            "       ____________________________________________________________________________________________________|_________________________________________________________________________________________________________________   \n",
            "      |         |                                                                                                S                                                                                                           | \n",
            "      |         |               _________________________________________________________________________________|_______________________                                                                                    |  \n",
            "      |         |              |             |                                                                                           VP                                                                                  | \n",
            "      |         |              |             |              _____________________________________________________________________________|_______________________                                                            |  \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |             VP                                                          | \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |       ______|___________                                                |  \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |                  PP                                              | \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |       ___________|____________                                   |  \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |                        NP                                 | \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |    ____________________|_______                           |  \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |   |       |                   SBAR                        | \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |   |       |        ____________|____                      |  \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |   |       |       |                 S                     | \n",
            "      |         |              |             |             |      |        |                  |                  |                             |   |      |      |   |       |       |         ________|_____                |  \n",
            "      |         |              |             |             |      |        |                  |                  NP                            |   |      |      |   |       |       |        |              VP              | \n",
            "      |         |              |             |             |      |        |                  |        __________|_____________                |   |      |      |   |       |       |        |         _____|_____          |  \n",
            "      |         |              |             |             |      |        |                  |       |          |             VP              |   |      |      |   |       |       |        |        |           VP        | \n",
            "      |         |              |             |             |      |        |                  |       |          |       ______|___            |   |      |      |   |       |       |        |        |      _____|____     |  \n",
            "      |         |             ADVP           |             |      |        PP                 |       |          |      |          PP          |   |      |      |   |       |       |        |        |     |          S    | \n",
            "      |         |          ____|____         |             |      |     ___|____              |       |          |      |       ___|_____      |   |      |      |   |       |       |        |        |     |          |    |  \n",
            "      NP        |         NP        |        NP            |     ADVP  |        NP            |       NP         |      |      |         NP    |   |      |      |   |       |       |        NP       |     |         ADJP  | \n",
            "  ____|____     |     ____|____     |    ____|_____        |      |    |    ____|______       |    ___|____      |      |      |         |     |   |      |      |   |       |       |     ___|___     |     |          |    |  \n",
            " CD        NN   CC   CD       NNS   RB PRP$       NNS     VBD     RB   IN  DT          NN     ,   DT  JJ   NN    ,     VBN     IN       NNP    ,   CC    VBN     IN  DT      NN      IN   DT     NNS  VBP   VBN         JJ   . \n",
            " |         |    |    |         |    |   |          |       |      |    |   |           |      |   |   |    |     |      |      |         |     |   |      |      |   |       |       |    |       |    |     |          |    |  \n",
            "Four     score and seven     years ago our      fathers brought forth  on this     continent  ,   a  new nation  ,  conceived  in     Liberty  ,  and dedicated  to the proposition that all     men  are created     equal  . \n",
            "\n",
            "\n",
            "Raw parse string:\n",
            "(TOP\n",
            "  (S\n",
            "    (NP (CD Four) (NN score))\n",
            "    (CC and)\n",
            "    (S\n",
            "      (ADVP (NP (CD seven) (NNS years)) (RB ago))\n",
            "      (NP (PRP$ our) (NNS fathers))\n",
            "      (VP\n",
            "        (VBD brought)\n",
            "        (ADVP (RB forth))\n",
            "        (PP (IN on) (NP (DT this) (NN continent)))\n",
            "        (, ,)\n",
            "        (NP\n",
            "          (NP (DT a) (JJ new) (NN nation))\n",
            "          (, ,)\n",
            "          (VP (VBN conceived) (PP (IN in) (NP (NNP Liberty)))))\n",
            "        (, ,)\n",
            "        (CC and)\n",
            "        (VP\n",
            "          (VBN dedicated)\n",
            "          (PP\n",
            "            (IN to)\n",
            "            (NP\n",
            "              (DT the)\n",
            "              (NN proposition)\n",
            "              (SBAR\n",
            "                (IN that)\n",
            "                (S\n",
            "                  (NP (DT all) (NNS men))\n",
            "                  (VP\n",
            "                    (VBP are)\n",
            "                    (VP (VBN created) (S (ADJP (JJ equal))))))))))))\n",
            "    (. .)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Extract Noun Phrases  \n",
        "Using spaCy, extract all noun phrases (noun chunks) from both sentences."
      ],
      "metadata": {
        "id": "pPsgBwvVfLwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Sentence 1 Noun Phrases ---\")\n",
        "for chunk in doc1.noun_chunks:\n",
        "    print(f\"  '{chunk.text}'  \\t root='{chunk.root.text}', dep='{chunk.root.dep_}'\")\n",
        "\n",
        "print(\"\\n--- Sentence 2 Noun Phrases ---\")\n",
        "for chunk in doc2.noun_chunks:\n",
        "    print(f\"  '{chunk.text}'  \\t root='{chunk.root.text}', dep='{chunk.root.dep_}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6vkat_ZeGqE",
        "outputId": "45f1402e-fc67-424b-d715-a93badc5b77e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence 1 Noun Phrases ---\n",
            "  'Four score'  \t root='score', dep='nsubj'\n",
            "  'our fathers'  \t root='fathers', dep='nsubj'\n",
            "  'this continent'  \t root='continent', dep='pobj'\n",
            "  'Liberty'  \t root='Liberty', dep='pobj'\n",
            "  'the proposition'  \t root='proposition', dep='pobj'\n",
            "  'all men'  \t root='men', dep='nsubjpass'\n",
            "\n",
            "--- Sentence 2 Noun Phrases ---\n",
            "  'we'  \t root='we', dep='nsubjpass'\n",
            "  'a great civil war'  \t root='war', dep='pobj'\n",
            "  'that nation'  \t root='nation', dep='nsubj'\n",
            "  'any nation'  \t root='nation', dep='conj'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. CRF and HMM  \n",
        "Why do you use CRF and HMM? How do they differ? Please summarize in less than 50\n",
        "words."
      ],
      "metadata": {
        "id": "-EIBLb9bfRaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM is a generative sequence model that estimates joint probability P(X,Y) using Markov assumptions. CRF is a discriminative model that directly estimates P(Y|X) and allows richer feature representation. CRFs typically perform better for structured prediction tasks like POS tagging and named entity recognition."
      ],
      "metadata": {
        "id": "kU3yuOF7ee_t"
      }
    }
  ]
}