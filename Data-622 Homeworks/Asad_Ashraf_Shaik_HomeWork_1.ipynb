{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Retrieve the following Web Page        \n",
        "Use the requests library to fetch the content of https://www.visitmaryland.org/      \n",
        "What is the HTTP status code?"
      ],
      "metadata": {
        "id": "2g1JFQ3fgWn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab12wfjTu5uA",
        "outputId": "ffeff459-4f8f-4e3d-870b-0177537d2c06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhD8b4m4u3Dt",
        "outputId": "8435dccd-bbec-4f70-bba0-d4bc8ad298df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP Status Code: 403\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.visitmaryland.org/\"\n",
        "resp = requests.get(url, timeout=30)\n",
        "\n",
        "print(\"HTTP Status Code:\", resp.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The requests library is used to send an HTTP GET request to the specified website. The server responds with a status code that indicates whether the request was successful. In this case, the response returns a 403 Forbidden status code because the website uses bot-protection mechanisms that block automated requests made by scripts instead of real browsers. This means the request reached the server, but access to the actual page content was denied."
      ],
      "metadata": {
        "id": "e3jR5hUoeYJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extract the Main Page Text\n",
        "Using BeautifulSoup, extract and print all the visible text from the page (ignore script and style tags)."
      ],
      "metadata": {
        "id": "ogKDYoJLgWEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "# Remove script and style elements\n",
        "for tag in soup([\"script\", \"style\"]):\n",
        "    tag.decompose()\n",
        "\n",
        "# Extract visible text\n",
        "text = soup.get_text(separator=\"\\n\")\n",
        "lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "print(\"\\n\".join(lines[:40]))\n"
      ],
      "metadata": {
        "id": "llkKYndWEBvL",
        "outputId": "b22ce27f-8e56-43f0-e6df-e166283b9ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just a moment...\n",
            "Enable JavaScript and cookies to continue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extracted text shows a Cloudflare bot-protection message instead of the actual website content. This occurs because the requests library does not execute JavaScript or manage browser cookies, which are required by the site. As a result, the server returns a challenge page rather than the real content."
      ],
      "metadata": {
        "id": "VjBVK7-Te4a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "wiki_url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "wiki_resp = requests.get(wiki_url, headers=headers, timeout=30)\n",
        "\n",
        "print(\"Status:\", wiki_resp.status_code)\n",
        "print(\"Final URL:\", wiki_resp.url)\n",
        "print(\"Length of HTML:\", len(wiki_resp.text))\n",
        "print(\"First 200 chars:\\n\", wiki_resp.text[:200])\n",
        "\n",
        "wiki_soup = BeautifulSoup(wiki_resp.text, \"html.parser\")\n",
        "\n",
        "headings = wiki_soup.select(\"h1, h2, h3\")\n",
        "links = wiki_soup.select(\"a[href]\")\n",
        "\n",
        "print(\"Headings found:\", len(headings))\n",
        "print(\"Links found:\", len(links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeHBuda2fo0w",
        "outputId": "63b82e25-7841-494c-98d6-d9dd11a5a6dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: 200\n",
            "Final URL: https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "Length of HTML: 308626\n",
            "First 200 chars:\n",
            " <!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pin\n",
            "Headings found: 22\n",
            "Links found: 969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract Headings from a Wikipedia Page\n",
        "Fetch the Wikipedia page for Natural Language Processing.\n",
        "Extract and print all the headings from the page."
      ],
      "metadata": {
        "id": "CsSZi0FygVA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headings = wiki_soup.select(\"h1, h2, h3\")\n",
        "\n",
        "for h in headings:\n",
        "    print(f\"{h.name}: {h.get_text(strip=True)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSbEKsSUe5dM",
        "outputId": "8d45db99-302d-4f27-924a-cedc400bc20e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h2: Contents\n",
            "h1: Natural language processing\n",
            "h2: History\n",
            "h3: Symbolic NLP (1950s – early 1990s)\n",
            "h3: Statistical NLP (1990s–present)\n",
            "h2: Approaches: Symbolic, statistical, neural networks\n",
            "h3: Statistical approach\n",
            "h3: Neural networks\n",
            "h2: Common NLP tasks\n",
            "h3: Text and speech processing\n",
            "h3: Morphological analysis\n",
            "h3: Syntactic analysis\n",
            "h3: Lexical semantics (of individual words in context)\n",
            "h3: Relational semantics (semantics of individual sentences)\n",
            "h3: Discourse (semantics beyond individual sentences)\n",
            "h3: Higher-level NLP applications\n",
            "h2: General tendencies and (possible) future directions\n",
            "h3: Cognition\n",
            "h2: See also\n",
            "h2: References\n",
            "h2: Further reading\n",
            "h2: External links\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extract Links from a Page\n",
        "From the Wikipedia page above, extract and print all the URLs (href values) of links found on the page."
      ],
      "metadata": {
        "id": "HUiCr0WTgUhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = wiki_soup.select(\"a[href]\")\n",
        "\n",
        "for a in links[:100]:\n",
        "    print(a[\"href\"])\n",
        "\n",
        "print(\"Total links:\", len(links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APT7izecfAPT",
        "outputId": "1bc1b27c-f83b-419f-e031-e60f0fc2c474"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#bodyContent\n",
            "/wiki/Main_Page\n",
            "/wiki/Wikipedia:Contents\n",
            "/wiki/Portal:Current_events\n",
            "/wiki/Special:Random\n",
            "/wiki/Wikipedia:About\n",
            "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
            "/wiki/Help:Contents\n",
            "/wiki/Help:Introduction\n",
            "/wiki/Wikipedia:Community_portal\n",
            "/wiki/Special:RecentChanges\n",
            "/wiki/Wikipedia:File_upload_wizard\n",
            "/wiki/Special:SpecialPages\n",
            "/wiki/Main_Page\n",
            "/wiki/Special:Search\n",
            "https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en\n",
            "/w/index.php?title=Special:CreateAccount&returnto=Natural+language+processing\n",
            "/w/index.php?title=Special:UserLogin&returnto=Natural+language+processing\n",
            "https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en\n",
            "/w/index.php?title=Special:CreateAccount&returnto=Natural+language+processing\n",
            "/w/index.php?title=Special:UserLogin&returnto=Natural+language+processing\n",
            "#\n",
            "#History\n",
            "#Symbolic_NLP_(1950s_–_early_1990s)\n",
            "#Statistical_NLP_(1990s–present)\n",
            "#Approaches:_Symbolic,_statistical,_neural_networks\n",
            "#Statistical_approach\n",
            "#Neural_networks\n",
            "#Common_NLP_tasks\n",
            "#Text_and_speech_processing\n",
            "#Morphological_analysis\n",
            "#Syntactic_analysis\n",
            "#Lexical_semantics_(of_individual_words_in_context)\n",
            "#Relational_semantics_(semantics_of_individual_sentences)\n",
            "#Discourse_(semantics_beyond_individual_sentences)\n",
            "#Higher-level_NLP_applications\n",
            "#General_tendencies_and_(possible)_future_directions\n",
            "#Cognition\n",
            "#See_also\n",
            "#References\n",
            "#Further_reading\n",
            "#External_links\n",
            "https://af.wikipedia.org/wiki/Natuurliketaalverwerking\n",
            "https://ar.wikipedia.org/wiki/%D9%85%D8%B9%D8%A7%D9%84%D8%AC%D8%A9_%D8%A7%D9%84%D9%84%D8%BA%D8%A9_%D8%A7%D9%84%D8%B7%D8%A8%D9%8A%D8%B9%D9%8A%D8%A9\n",
            "https://hyw.wikipedia.org/wiki/%D4%B2%D5%B6%D5%A1%D5%AF%D5%A1%D5%B6_%D4%BC%D5%A5%D5%A6%D5%B8%D6%82%D5%AB_%D5%84%D5%B7%D5%A1%D5%AF%D5%B8%D6%82%D5%B4\n",
            "https://az.wikipedia.org/wiki/T%C9%99bii_dilin_emal%C4%B1\n",
            "https://bn.wikipedia.org/wiki/%E0%A6%B8%E0%A7%8D%E0%A6%AC%E0%A6%BE%E0%A6%AD%E0%A6%BE%E0%A6%AC%E0%A6%BF%E0%A6%95_%E0%A6%AD%E0%A6%BE%E0%A6%B7%E0%A6%BE_%E0%A6%AA%E0%A7%8D%E0%A6%B0%E0%A6%95%E0%A7%8D%E0%A6%B0%E0%A6%BF%E0%A6%AF%E0%A6%BC%E0%A6%BE%E0%A6%9C%E0%A6%BE%E0%A6%A4%E0%A6%95%E0%A6%B0%E0%A6%A3\n",
            "https://zh-min-nan.wikipedia.org/wiki/Ch%C5%AB-ji%C3%A2n_gi%C3%A2n-g%C3%BA_chh%C3%BA-l%C3%AD\n",
            "https://be.wikipedia.org/wiki/%D0%90%D0%BF%D1%80%D0%B0%D1%86%D0%BE%D1%9E%D0%BA%D0%B0_%D0%BD%D0%B0%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D0%B9_%D0%BC%D0%BE%D0%B2%D1%8B\n",
            "https://be-tarask.wikipedia.org/wiki/%D0%90%D0%BF%D1%80%D0%B0%D1%86%D0%BE%D1%9E%D0%BA%D0%B0_%D0%BD%D0%B0%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D0%B9_%D0%BC%D0%BE%D0%B2%D1%8B\n",
            "https://bg.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD_%D0%B5%D0%B7%D0%B8%D0%BA\n",
            "https://bs.wikipedia.org/wiki/Obrada_prirodnog_jezika\n",
            "https://br.wikipedia.org/wiki/Treterezh_emgefre_al_lavar\n",
            "https://ca.wikipedia.org/wiki/Processament_del_llenguatge_natural\n",
            "https://cs.wikipedia.org/wiki/Zpracov%C3%A1n%C3%AD_p%C5%99irozen%C3%A9ho_jazyka\n",
            "https://cy.wikipedia.org/wiki/Prosesu_Iaith_Naturiol\n",
            "https://da.wikipedia.org/wiki/Sprogteknologi\n",
            "https://de.wikipedia.org/wiki/Verarbeitung_nat%C3%BCrlicher_Sprache\n",
            "https://et.wikipedia.org/wiki/Loomuliku_keele_t%C3%B6%C3%B6tlus\n",
            "https://el.wikipedia.org/wiki/%CE%95%CF%80%CE%B5%CE%BE%CE%B5%CF%81%CE%B3%CE%B1%CF%83%CE%AF%CE%B1_%CF%86%CF%85%CF%83%CE%B9%CE%BA%CE%AE%CF%82_%CE%B3%CE%BB%CF%8E%CF%83%CF%83%CE%B1%CF%82\n",
            "https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales\n",
            "https://eo.wikipedia.org/wiki/Natur-lingva_prilaborado\n",
            "https://eu.wikipedia.org/wiki/Hizkuntzaren_prozesamendu\n",
            "https://fa.wikipedia.org/wiki/%D9%BE%D8%B1%D8%AF%D8%A7%D8%B2%D8%B4_%D8%B2%D8%A8%D8%A7%D9%86%E2%80%8C%D9%87%D8%A7%DB%8C_%D8%B7%D8%A8%DB%8C%D8%B9%DB%8C\n",
            "https://fr.wikipedia.org/wiki/Traitement_automatique_des_langues\n",
            "https://ga.wikipedia.org/wiki/Pr%C3%B3ise%C3%A1il_teanga_n%C3%A1d%C3%BArtha\n",
            "https://gl.wikipedia.org/wiki/Procesamento_da_linguaxe_natural\n",
            "https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC\n",
            "https://hy.wikipedia.org/wiki/%D4%B2%D5%B6%D5%A1%D5%AF%D5%A1%D5%B6_%D5%AC%D5%A5%D5%A6%D5%BE%D5%AB_%D5%B4%D5%B7%D5%A1%D5%AF%D5%B8%D6%82%D5%B4\n",
            "https://hi.wikipedia.org/wiki/%E0%A4%AA%E0%A5%8D%E0%A4%B0%E0%A4%BE%E0%A4%95%E0%A5%83%E0%A4%A4%E0%A4%BF%E0%A4%95_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE_%E0%A4%B8%E0%A4%82%E0%A4%B8%E0%A4%BE%E0%A4%A7%E0%A4%A8\n",
            "https://hr.wikipedia.org/wiki/Obrada_prirodnog_jezika\n",
            "https://io.wikipedia.org/wiki/Procedo_pri_naturala_linguo\n",
            "https://id.wikipedia.org/wiki/Pengolahan_bahasa_alami\n",
            "https://zu.wikipedia.org/wiki/Ukudludlunga_ulimi_lwemvelo\n",
            "https://is.wikipedia.org/wiki/M%C3%A1lgreining\n",
            "https://it.wikipedia.org/wiki/Elaborazione_del_linguaggio_naturale\n",
            "https://he.wikipedia.org/wiki/%D7%A2%D7%99%D7%91%D7%95%D7%93_%D7%A9%D7%A4%D7%94_%D7%98%D7%91%D7%A2%D7%99%D7%AA\n",
            "https://kn.wikipedia.org/wiki/%E0%B2%B8%E0%B2%B9%E0%B2%9C_%E0%B2%AD%E0%B2%BE%E0%B2%B7%E0%B2%BE_%E0%B2%B8%E0%B2%82%E0%B2%B8%E0%B3%8D%E0%B2%95%E0%B2%B0%E0%B2%A3%E0%B3%86\n",
            "https://ka.wikipedia.org/wiki/%E1%83%91%E1%83%A3%E1%83%9C%E1%83%94%E1%83%91%E1%83%A0%E1%83%98%E1%83%95%E1%83%98_%E1%83%94%E1%83%9C%E1%83%98%E1%83%A1_%E1%83%93%E1%83%90%E1%83%9B%E1%83%A3%E1%83%A8%E1%83%90%E1%83%95%E1%83%94%E1%83%91%E1%83%90\n",
            "https://lv.wikipedia.org/wiki/Dabisk%C4%81s_valodas_apstr%C4%81de\n",
            "https://lt.wikipedia.org/wiki/Nat%C5%ABraliosios_kalbos_apdorojimas\n",
            "https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8\n",
            "https://mr.wikipedia.org/wiki/%E0%A4%A8%E0%A5%88%E0%A4%B8%E0%A4%B0%E0%A5%8D%E0%A4%97%E0%A4%BF%E0%A4%95_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE_%E0%A4%AA%E0%A5%8D%E0%A4%B0%E0%A4%95%E0%A5%8D%E0%A4%B0%E0%A4%BF%E0%A4%AF%E0%A4%BE\n",
            "https://arz.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%D9%8A%D9%84_%D8%A7%D9%84%D9%84%D8%BA%D8%A7%D8%AA_%D8%A7%D9%84%D8%B7%D8%A8%D9%8A%D8%B9%D9%8A%D9%87\n",
            "https://mn.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D1%8B%D0%BD_%D1%85%D1%8D%D0%BB_%D1%88%D0%B8%D0%BD%D0%B6%D0%BB%D1%8D%D0%BB\n",
            "https://my.wikipedia.org/wiki/%E1%80%99%E1%80%AD%E1%80%81%E1%80%84%E1%80%BA%E1%80%98%E1%80%AC%E1%80%9E%E1%80%AC%E1%80%85%E1%80%80%E1%80%AC%E1%80%B8%E1%80%9E%E1%80%AF%E1%80%B6%E1%80%B8_%E1%80%80%E1%80%BD%E1%80%94%E1%80%BA%E1%80%95%E1%80%BB%E1%80%B0%E1%80%90%E1%80%AC%E1%80%85%E1%80%94%E1%80%85%E1%80%BA\n",
            "https://nl.wikipedia.org/wiki/Taaltechnologie\n",
            "https://ja.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86\n",
            "https://no.wikipedia.org/wiki/Spr%C3%A5kteknologi\n",
            "https://or.wikipedia.org/wiki/%E0%AC%A8%E0%AD%8D%E0%AD%9F%E0%AC%BE%E0%AC%9A%E0%AD%81%E0%AC%B0%E0%AC%BE%E0%AC%B2_%E0%AC%B2%E0%AC%BE%E0%AC%99%E0%AD%8D%E0%AC%97%E0%AD%81%E0%AC%8F%E0%AC%9C_%E0%AC%AA%E0%AD%8D%E0%AC%B0%E0%AD%8B%E0%AC%B8%E0%AD%87%E0%AC%B8%E0%AC%BF%E0%AC%82\n",
            "https://ps.wikipedia.org/wiki/%D8%AF_%D8%B7%D8%A8%D9%8A%D8%B9%D9%8A_%DA%98%D8%A8%DB%90_%D9%BE%D8%B1%D9%88%D8%B3%D8%B3_%DA%A9%D9%88%D9%84\n",
            "https://pcd.wikipedia.org/wiki/Traitemint_automatique_d%27ches_langues\n",
            "https://pms.wikipedia.org/wiki/NLP\n",
            "https://pl.wikipedia.org/wiki/Przetwarzanie_j%C4%99zyka_naturalnego\n",
            "https://pt.wikipedia.org/wiki/Processamento_de_linguagem_natural\n",
            "https://kaa.wikipedia.org/wiki/T%C3%A1biyiy_tildi_qayta_islew\n",
            "https://ro.wikipedia.org/wiki/Prelucrarea_limbajului_natural\n",
            "https://qu.wikipedia.org/wiki/Paqariq_simi_thatkichay\n",
            "https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0\n",
            "https://sq.wikipedia.org/wiki/P%C3%ABrpunimi_i_gjuh%C3%ABs_natyrore\n",
            "Total links: 969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Extract and Save Text\n",
        "Extract the first paragraph(< p >) from the Wikipedia page and save it to a local text file called nlp_intro.txt."
      ],
      "metadata": {
        "id": "72sX6qQ_gTkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_p = wiki_soup.find(\"p\")\n",
        "first_paragraph_text = first_p.get_text(strip=True) if first_p else \"\"\n",
        "\n",
        "with open(\"nlp_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(first_paragraph_text)\n",
        "\n",
        "print(\"Saved first paragraph to nlp_intro.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JwOssaRfBht",
        "outputId": "8ab16401-51f2-4dbe-c0b4-3ea542199247"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved first paragraph to nlp_intro.txt\n"
          ]
        }
      ]
    }
  ]
}